##### 1. SVM
- 拉格朗日乘子法
- 原始问题和对偶问题？对偶问题的好处？
- KKT 条件
- SVM 怎么防止过拟合
- 合页损失函数是针对原始问题还是对偶问题？

##### 2. LR
- LR 为什么用 sigmoid 函数？这个函数有什么优点和缺点？为什么不用其他函数？
- LR 和 SVM 区别？
- LR 和线性回归区别？

##### 3. 树模型
1）随机森林怎么防止过拟合

2）XGBoost 与传统 GBDT 的区别？怎么做并行.

##### 4. 正则
- 为什么正则化可以防止过拟合？
- L1 正则为什么可以把系数压缩成 0？
- 0.5 正则


##### 5. 优化方法
- 坐标下降法的具体实现细节
- 为什么负梯度方向是函数值下降最快的方向？
- 随机梯度下降为什么能够收敛到最小值点？


参考资料：

1）[为什么梯度反方向是函数值下降最快的方向？](https://zhuanlan.zhihu.com/p/24913912)

##### 6. 聚类方法
1）k-means；以及与 EM 算法的关系.

参考：
- [k-means 聚类算法](http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006910.html)

##### 7. 特征选择的方法

##### 8. 过拟合的解决方法

##### 9. 机器学习基础问题
1）VC 维

2）偏差-方差分解

##### 其他问题
- 树形结构，SVM，LR 都适用什么场景？Boosting，随机森林适用于什么场景？
- 有一堆已经分好的词，如何发现新词？

用这个词和左右词的关系。互信息 新词的左右比较丰富，有的老词的左右也比较丰富。还要区分出新词和老词。

参考：[反作弊基于左右信息熵和互信息的新词挖掘](https://zhuanlan.zhihu.com/p/25499358?hmsr=toutiao.io&utm_medium=toutiao.io&utm_source=toutiao.io)


---

今日头条算法工程师

编程题：矩阵

1）LR 和线性回归的区别？线性回归可以用来做正负分类么？

2）LR 和 SVM 损失函数推导；

3）LR 和 SVM 区别？

4）SVM 中的 hinge 损失是对于原始问题还是对偶问题？

5）SVM 怎么优化？SMO 算法的时间复杂度和空间复杂度？

6）SVM 为什么不适合处理大数据？SMO 也不行？为什么适合小样本多特征？LR 适用什么情况？

7）树形结构适用于什么场景？boosting 和随机森林适用于什么场景？

8）为什么 L1 正则可以获得稀疏解？L0.5 正则可以获得稀疏解么？L1.5 正则呢？
